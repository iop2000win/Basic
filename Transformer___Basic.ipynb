{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f71a426-ff61-4f0b-9900-95b4690b7b87",
   "metadata": {},
   "source": [
    "https://wikidocs.net/166802\n",
    "\n",
    "https://huggingface.co/learn/nlp-course/ko/chapter2/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a32a1147-ccc8-4c7a-a0c8-0bb4502643fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ad7da9b-72ca-4c09-976a-279d5ae37e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a22198c5-51d8-47d9-b385-c7f2c8b18ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16ddf33d55b4ed38a68aa5d3b199c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e329727-7dc6-40c6-859d-2df4a491fc5a",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b9617cf-5cf4-4fcc-a7c8-d7547ab6b1aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e779da3e8d47d5851792946ff70a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/6.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafd1b4f5a414c8c896b6d832f82a79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/299M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6794448e5f6e4d93853f6470d0b2a8fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/23.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7947008c6da546b0aca0eacf4ad1037c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/650000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972a0fa340fd4fbe81c32e93bfc9fd42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('yelp_review_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3293632-6eef-44e4-983d-39c5f00cff68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2679612f-b16b-4f51-a183-e0e16ed401ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\nThe cashier took my friends's order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid's meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\"serving off their orders\\\" when they didn't have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\nThe manager was rude when giving me my order. She didn't make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\nI've eaten at various McDonalds restaurants for over 30 years. I've worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!\n"
     ]
    }
   ],
   "source": [
    "s = dataset['train'][100]\n",
    "\n",
    "print(s['label'])\n",
    "print(s['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "813f1615-5e2a-4cc4-92f6-1172ad4c795e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "59080043-d693-4a98-810c-136581cfc92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527645d6257e4d4c853dde291e0d4c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/650000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65493508317040258bb2b67db4aaa986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-cased')\n",
    "\n",
    "def tokenize_func(examples):\n",
    "    return tokenizer(examples['text'], padding = 'max_length', truncation = True)\n",
    "\n",
    "# dataset에 있는 map 메서드를 통해 전체 데이터에 토크나이징을 적용할 수 있음\n",
    "tokenized_datasets = dataset.map(tokenize_func, batched = True) # 배치 단위로 mapping을 진행하는 옵션으로 속도면에서 성능을 발휘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "664ff766-546f-4736-a774-a8bc132f2255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연습을 위한 예제 데이터 셋을 더 작은 규모로 설정\n",
    "small_train_dataset = tokenized_datasets['train'].shuffle(seed = 42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets['test'].shuffle(seed = 42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bd646c-34d8-4c2b-b91e-8f77c428db7c",
   "metadata": {},
   "source": [
    "### Pytorch Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "901b7e3e-263d-4cb6-b840-d2ae9c899c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a7697710-d8a6-46a8-b1f3-5df4c1eaa7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b901bcab1c34092b4897254b373ed25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('google-bert/bert-base-cased', num_labels = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "41cc2b7d-fc5e-499b-9e07-242987adadff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir = 'test_trainer') # 체크포인트 저장 경로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c49a8c11-e890-4ec3-93fa-c2533f306969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 평가를 위한 작업 (케라스, 텐서플로우와 달리 학습 과정에서 바로 정확도가 산출되지 않는다.)\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis = -1)\n",
    "\n",
    "    return metric.compute(predictions = predictions, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "da56b41e-af3e-44d3-af5e-d334fb3245c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'test_trainer',\n",
    "    evaluation_strategy = 'epoch',\n",
    "    num_train_epochs = , # default = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "87315eb3-cf1f-4ef6-989b-be7fc28298d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = small_train_dataset,\n",
    "    eval_dataset = small_eval_dataset,\n",
    "    compute_metrics = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e9781180-58af-4734-896e-1a0f6336a799",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 02:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.079821</td>\n",
       "      <td>0.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.002188</td>\n",
       "      <td>0.576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.050602</td>\n",
       "      <td>0.583000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.9944601236979167, metrics={'train_runtime': 129.4787, 'train_samples_per_second': 23.17, 'train_steps_per_second': 2.896, 'total_flos': 789354427392000.0, 'train_loss': 0.9944601236979167, 'epoch': 3.0})"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7d7369bc-80e4-48da-9546-4ef96a4e5855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0506024360656738,\n",
       " 'eval_accuracy': 0.583,\n",
       " 'eval_runtime': 10.3182,\n",
       " 'eval_samples_per_second': 96.916,\n",
       " 'eval_steps_per_second': 12.114,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7c8ebd-5cf7-4cb9-8dd6-e4be59cd0c8f",
   "metadata": {},
   "source": [
    "### Keras Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2fe75893-1367-42c7-90b9-bc76794acd98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a9dbad2a4f48ed990b8a7dace648d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/35.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2910e7bff99b4fa2b015f5ce06a6a642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/251k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ad7f0ab40540119281c5eab5496ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/37.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c285f2a68534aea86d84b286021f854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/37.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a22868d63046118b93dfbf4bef8df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b795430ff9cd4f2da5b9c4c744f97ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1043 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d342404b5794784ab41803782425edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터 형식 변환이 필요함\n",
    "dataset = load_dataset('glue', 'cola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "696a44ce-b908-4541-b6b0-80804db68b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "valid_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "526362a1-4b77-471c-bce9-478e819da13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8551, 1043, 1063)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9b539a20-296f-4cde-b5e3-d5e99e25f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-cased')\n",
    "tokenized_data = tokenizer(train_dataset['sentence'], return_tensors = 'np', padding = True)\n",
    "\n",
    "tokenized_data = dict(tokenized_data)\n",
    "\n",
    "labels = np.array(train_dataset['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d525dbdf-ecda-45ca-ac62-c3fbd979cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam # 최적화 함수 설정\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained('google-bert/bert-base-cased')\n",
    "model.compile(optimizer = Adam(3e-5))\n",
    "\n",
    "model.fit(tokenized_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7b682d-37c3-4ce5-9aa9-443c6686e36b",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f9fb3a4-9e67-45f7-a426-553d7db887bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent_kor = '종근당건강 락토핏 솔루션 2 예민한 장 450mg 30캡슐 X 3박스 3개월분'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c144d182-b4e4-490e-aabd-fea415cc4009",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'klue/roberta-small'\n",
    "\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "tokenizer_auto = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "tokenizer_hug = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d903c865-5ee2-4b66-b911-c2dae6977902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "882eb5cc-6e16-44e7-b209-61a5bff75bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 100, 100, 100, 123, 100, 100, 10181, 1306, 1403, 100, 161, 100, 100, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[CLS] [UNK] [UNK] [UNK] 2 [UNK] [UNK] 450mg [UNK] X [UNK] [UNK] [SEP]\n"
     ]
    }
   ],
   "source": [
    "incode = tokenizer_bert(test_sent_kor)\n",
    "print(incode)\n",
    "\n",
    "decode = tokenizer_bert.decode(incode['input_ids'])\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b234313-3871-4f10-b083-0d58e27eae06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "448066ab-d3b1-4142-b1c0-7bebba70e023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 100, 100, 100, 123, 100, 100, 10181, 1306, 1403, 100, 161, 100, 100, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[CLS] [UNK] [UNK] [UNK] 2 [UNK] [UNK] 450mg [UNK] X [UNK] [UNK] [SEP]\n"
     ]
    }
   ],
   "source": [
    "incode = tokenizer_auto(test_sent_kor)\n",
    "print(incode)\n",
    "\n",
    "decode = tokenizer_auto.decode(incode['input_ids'])\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f411d84e-e6f9-4978-aa94-25a170caec33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='klue/roberta-small', vocab_size=32000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_hug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7dbe8965-0141-4074-b6da-ccb8887383a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 1558, 2169, 2481, 2332, 2280, 943, 2386, 2946, 8463, 22, 11028, 2470, 1526, 13103, 2037, 2064, 3740, 2941, 3381, 60, 23, 13473, 23, 2019, 2429, 2377, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[CLS] 종근당건강 락토핏 솔루션 2 예민한 장 450mg 30캡슐 X 3박스 3개월분 [SEP]\n"
     ]
    }
   ],
   "source": [
    "incode = tokenizer_hug(test_sent_kor)\n",
    "print(incode)\n",
    "\n",
    "decode = tokenizer_hug.decode(incode['input_ids'])\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaece215-4b2f-484e-91aa-830561e5d9cb",
   "metadata": {},
   "source": [
    "### 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "31e78757-6f25-4f38-a054-c00e52a1593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = 'using Using a Transformer network is simple'\n",
    "tokens = tokenizer_hug.tokenize(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5d147f6a-dd89-40bc-a1a4-e36b9e5ef82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 ['us', '##ing', 'U', '##s', '##ing', 'a', 'Trans', '##form', '##er', 'net', '##work', 'is', 's', '##im', '##ple']\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens), tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "99d0a88b-5c60-4675-967e-f3d2dc3f2d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 [0, 26099, 4586, 57, 2041, 4586, 68, 23877, 16240, 3762, 17640, 24384, 11376, 86, 6828, 31439, 2]\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer_hug(seq)['input_ids']), tokenizer_hug(seq)['input_ids'])\n",
    "# [CLS], tokens... , [SEP] 해서 token보다 2개 갯수 더 많은 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c3905b9d-45ac-4829-8608-e52b881a3e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26099, 4586, 57, 2041, 4586, 68, 23877, 16240, 3762, 17640, 24384, 11376, 86, 6828, 31439]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer_hug.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)\n",
    "# 이러한 아이디 값들을 텐서로 변환하는 단계가 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020fb3e2-0f6e-4dbe-9e29-ec47b37d41c4",
   "metadata": {},
   "source": [
    "### 디코딩\n",
    "\n",
    "토큰들을 병합하여, 읽을 수 있는 원본 문장을 도출하는 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "81abd6cb-06ee-4d57-bbe2-cee22c587176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Using a Transformer network is simple\n",
      "[CLS] using Using a Transformer network is simple [SEP]\n"
     ]
    }
   ],
   "source": [
    "decoded_string = tokenizer_hug.decode(ids)\n",
    "\n",
    "print(decoded_string)\n",
    "print(tokenizer_hug.decode(tokenizer_hug(seq)['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9734f-0f45-472e-8514-abfced4b13d4",
   "metadata": {},
   "source": [
    "### 다중 시퀀스 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "41fad71d-4259-42d5-b349-8d62f8c75c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 26099,  4586,    57,  2041,  4586,    68, 23877, 16240,  3762,\n",
      "         17640, 24384, 11376,    86,  6828, 31439,     2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# return_tensors = 'pt'를 입력해주면, 결과값을 텐서로 리턴 받음\n",
    "tokenized_inputs = tokenizer_hug(seq, return_tensors = 'pt')\n",
    "print(tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5b790e33-6f06-475f-906d-3f4e2f95ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c621be14-10f2-492c-bd03-ab29c616ea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d515f7ed-6e03-4732-830e-083f2af791f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['using', 'using', 'a', 'transform', '##er', 'network', 'is', 'simple']\n",
      "[2478, 2478, 1037, 10938, 2121, 2897, 2003, 3722]\n",
      "tensor([ 2478,  2478,  1037, 10938,  2121,  2897,  2003,  3722])\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(seq)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor(ids)\n",
    "\n",
    "print(tokens)\n",
    "print(ids)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "10c5b28e-4af5-4411-b68f-9ddcd05ad335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2478,  2478,  1037, 10938,  2121,  2897,  2003,  3722,   102]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_inputs = tokenizer(seq, return_tensors = 'pt')\n",
    "\n",
    "print(tokenized_inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "47435517-4d40-42a9-b344-311a1cdfff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차원 형태 변형\n",
    "input_ids = torch.tensor([ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ee2b567c-78da-40bb-b9ce-7307a230a1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0906, -2.6761]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5fdba2-ca74-489f-b8ac-52ea93f3ad39",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d76e99c9-25fc-4976-83a0-b25d8a4124e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bathced_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200]\n",
    "]\n",
    "\n",
    "# 여러 개의 텍스트 데이터에 대해서, 각 데이터들을 토큰화 했을 때, 데이터의 사이즈가 달라진다.\n",
    "# 이를 해결하기 위한 방법이, 1. Padding 2. Truncating 가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "96f568e4-a333-4ce3-afc9-f3265d2b3a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9100fd7f-bfae-4537-b39d-14352f91002a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "seq_ids_1 = [[200, 200, 200]]\n",
    "seq_ids_2 = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(seq_ids_1)).logits)\n",
    "print(model(torch.tensor(seq_ids_2)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9f02cb-29c8-49cc-9448-650b5551eca1",
   "metadata": {},
   "source": [
    "### Transformer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d60a8f30-10dc-4db9-9655-0ca6833dd2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# list로 여러개의 텍스트도 입력 가능\n",
    "seq = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "seq_list = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"So have I!\"\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer(seq_list) # __call__ 함수를 통해 작동\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f7ffe05b-b6d4-42f4-b0c4-ad795bef2612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer(seq_list, padding = 'longest')\n",
    "print(model_inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
